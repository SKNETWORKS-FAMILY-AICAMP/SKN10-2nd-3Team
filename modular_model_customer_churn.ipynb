{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86uCi2XhMAt6"
   },
   "source": [
    "# Modular 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1740367141519,
     "user": {
      "displayName": "박예슬 (Yeseul Park)",
      "userId": "06789135698482112547"
     },
     "user_tz": -540
    },
    "id": "GzNllTpeJA2g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sPlyCDzsXN1"
   },
   "source": [
    "```python\n",
    "\n",
    "modular_model/ # 프로젝트 폴더\n",
    "├── service/\n",
    "│   ├── utils.py\n",
    "│   ├── dataset.py # 데이터\n",
    "│   ├── model.py # 모델\n",
    "│   └── run.py # 학습 프로세스\n",
    "└── data/ # 학습 데이터\n",
    "    ├── train.csv\n",
    "    ├── test.csv\n",
    "    └── submission.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2ib9yM6MHqI"
   },
   "source": [
    "## 폴더 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740367143780,
     "user": {
      "displayName": "박예슬 (Yeseul Park)",
      "userId": "06789135698482112547"
     },
     "user_tz": -540
    },
    "id": "VQqcJnH1sGwk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for dir in ['service', 'data']:\n",
    "    # 만약 해당 폴더가 없다면,\n",
    "    if not os.path.exists(dir):\n",
    "        # 해당 폴더를 만들어줘\n",
    "        os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1740367143807,
     "user": {
      "displayName": "박예슬 (Yeseul Park)",
      "userId": "06789135698482112547"
     },
     "user_tz": -540
    },
    "id": "h1PRgH5E3weJ",
    "outputId": "d431654e-2615-4e6e-99fc-75570a8aa380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting service/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile service/config.py\n",
    "\n",
    "import os\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "MODEL_DIR = os.path.join(DATA_DIR, 'models')\n",
    "\n",
    "# 모델 설정\n",
    "MODEL_VERSION = 'v1_rf'\n",
    "MODEL_FILENAME = f'model_{MODEL_VERSION}.pkl'\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "DATA_PATH = os.path.join(DATA_DIR, 'raw.csv')\n",
    "#TEST_PATH = os.path.join(DATA_DIR, 'test.csv')\n",
    "#SUBMISSION_PATH = os.path.join(DATA_DIR, f'submission_{MODEL_VERSION}.csv')\n",
    "\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nv6zD64MJwy"
   },
   "source": [
    "## utils 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1740367143938,
     "user": {
      "displayName": "박예슬 (Yeseul Park)",
      "userId": "06789135698482112547"
     },
     "user_tz": -540
    },
    "id": "Jq0e2RgAvlqT",
    "outputId": "ae18e17d-93b4-4c54-ea55-733064d17c82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting service/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile service/utils.py\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def reset_seeds(func, seed=42):\n",
    "  random.seed(seed)\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)    # 파이썬 환경변수 시드 고정\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed) # cpu 연산 무작위 고정\n",
    "  torch.cuda.manual_seed(seed) # gpu 연산 무작위 고정\n",
    "  torch.backends.cudnn.deterministic = True  # cuda 라이브러리에서 Deterministic(결정론적)으로 예측하기 (예측에 대한 불확실성 제거 )\n",
    "\n",
    "  def wrapper_func(*args, **kwargs):\n",
    "    return func(*args, **kwargs)\n",
    "\n",
    "  return wrapper_func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18k7MtfSMOl8"
   },
   "source": [
    "## dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1740367796506,
     "user": {
      "displayName": "박예슬 (Yeseul Park)",
      "userId": "06789135698482112547"
     },
     "user_tz": -540
    },
    "id": "7f_GALx42Hxv",
    "outputId": "9ab7ec80-35f9-4d4a-eefe-8ff5730f1bd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting service/dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile service/dataset.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import reset_seeds\n",
    "from config import DATA_PATH\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 트레인 데이터 로드\n",
    "def __load_data() -> pd.DataFrame:\n",
    "    data = pd.read_csv(DATA_PATH)\n",
    "    data = data.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    return data\n",
    "\n",
    "def __process_drop(train, val, test):\n",
    "    drop_cols = ['customerID','StreamingTV', 'StreamingMovies',\n",
    "                 'OnlineSecurity', 'OnlineBackup','DeviceProtection',]\n",
    "\n",
    "    train.drop(drop_cols, axis=1, inplace=True) # 모델이 학습하는데 사용하는 데이터\n",
    "    val.drop(drop_cols, axis=1, inplace=True) # 모델의 학습을 평가(잘했는지?? 못했는지??)하기 위한 데이터\n",
    "    test.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "def __fill_na(train, val, test):\n",
    "    null_cols = ['TotalCharges']\n",
    "    for df in [train, val, test]:\n",
    "      for col in null_cols:\n",
    "        df[col] = df[col].astype(float)\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "    return train, val, test\n",
    "\n",
    "def __preprocess_resample(train, val, test):\n",
    "    print(\"__preprocess_resample start\")\n",
    "    print(f\"train.shape: {train.shape} / test.shape: {val.shape}\")\n",
    "    X_train, y_train = SMOTE().fit_resample(train.drop(['Churn'], axis=1), train['Churn'])\n",
    "    X_val, y_val = SMOTE().fit_resample(val.drop(['Churn'], axis=1), val['Churn'])\n",
    "    X_test = test.drop(['Churn'], axis=1)\n",
    "    y_test = test['Churn']\n",
    "\n",
    "    print(\"__preprocess_resample end\")\n",
    "    print(f\"X_train.shape: {X_train.shape} / X_test.shape: {X_val.shape}\")\n",
    "    return X_train, X_val, y_train, y_val, X_test, y_test\n",
    "\n",
    "def __preprocess_label_encoding(train, val, test):\n",
    "    results = []\n",
    "\n",
    "    cat_features = []\n",
    "\n",
    "    # Remove categorical features from normal columns\n",
    "    normal_cols = list(set(train.columns) - set(cat_features))\n",
    "\n",
    "    # Initialize dictionary to store label encoders\n",
    "    label_encoders = {}\n",
    "\n",
    "    # Fit label encoders on training data and transform all datasets\n",
    "    encoded_features = {}\n",
    "    for feature in cat_features:\n",
    "        label_encoders[feature] = LabelEncoder()\n",
    "        # Fit on training data\n",
    "        encoded_features[feature] = label_encoders[feature].fit_transform(train[feature])\n",
    "\n",
    "    pd_list = [train, val, test]\n",
    "    for i, df in enumerate(pd_list):\n",
    "        # Create a copy of the dataframe\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        # Transform categorical features\n",
    "        for feature in cat_features:\n",
    "            try:\n",
    "                temp_df[feature] = label_encoders[feature].transform(df[feature])\n",
    "            except ValueError as e:\n",
    "                # Handle unseen categories in validation/test set\n",
    "                print(f\"Warning: Found unseen labels in {feature} for dataset {i+1}\")\n",
    "                # Get unique values in current dataset\n",
    "                unique_vals = df[feature].unique()\n",
    "                # Find values not in training set\n",
    "                unseen_vals = [x for x in unique_vals if x not in label_encoders[feature].classes_]\n",
    "                if unseen_vals:\n",
    "                    print(f\"Unseen values in {feature}: {unseen_vals}\")\n",
    "                    # Replace unseen values with the most frequent value from training\n",
    "                    most_frequent = train[feature].mode()[0]\n",
    "                    temp_df.loc[df[feature].isin(unseen_vals), feature] = most_frequent\n",
    "                    # Transform again after replacing unseen values\n",
    "                    temp_df[feature] = label_encoders[feature].transform(temp_df[feature])\n",
    "\n",
    "        # Only select columns that exist in current dataframe\n",
    "        available_cols = sorted([col for col in normal_cols if col in df.columns])\n",
    "\n",
    "        # Combine all features\n",
    "        result_df = temp_df[available_cols + cat_features].copy()\n",
    "        results.append(result_df.reset_index(drop=True))\n",
    "\n",
    "    return results[0], results[1], results[2]\n",
    "\n",
    "\n",
    "def __preprocess_dummy_encoding(train, val, test):\n",
    "    results = []\n",
    "\n",
    "    cat_features = ['PaymentMethod',\n",
    "                    'MultipleLines', 'InternetService', 'Contract',\n",
    "                    'TechSupport', ]\n",
    "\n",
    "    # Remove target from normal_cols calculation\n",
    "    normal_cols = list(set(train.columns) - set(cat_features))\n",
    "\n",
    "    # Get dummy variables for categorical features in training set\n",
    "    dummies_train = pd.get_dummies(train[cat_features], prefix=cat_features)\n",
    "\n",
    "    # Get dummy column names from training set for consistent columns across sets\n",
    "    dummy_columns = dummies_train.columns\n",
    "\n",
    "    pd_list = [train, val, test]\n",
    "    for i, df in enumerate(pd_list, start=1):\n",
    "        # Create dummies with only columns that were in training data\n",
    "        dummies_df = pd.get_dummies(df[cat_features], prefix=cat_features)\n",
    "\n",
    "        # Ensure all dummy columns from training are present\n",
    "        for col in dummy_columns:\n",
    "            if col not in dummies_df.columns:\n",
    "                dummies_df[col] = 0\n",
    "\n",
    "        # Keep only dummy columns from training (in case test has categories not in train)\n",
    "        dummies_df = dummies_df[dummy_columns]\n",
    "\n",
    "        # Only select columns that exist in current dataframe\n",
    "        available_cols = sorted([col for col in normal_cols if col in df.columns])\n",
    "\n",
    "        # Concatenate original features with dummy variables\n",
    "        results.append(\n",
    "            pd.concat(\n",
    "                [df[available_cols].reset_index(drop=True), dummies_df.reset_index(drop=True)],\n",
    "                axis=1\n",
    "            ).reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    return results[0], results[1], results[2]\n",
    "\n",
    "\n",
    "\n",
    "def __preprocess_yn (train, val, test):\n",
    "  yn_categories = ['gender', 'Partner', 'Dependents','PhoneService', 'PaperlessBilling','Churn']\n",
    "  for category in yn_categories:\n",
    "    train[category] = train[category].apply(lambda x: 1 if x in ['Yes','Female'] else 0)\n",
    "    val[category] = val[category].apply(lambda x: 1 if x in ['Yes','Female'] else 0)\n",
    "    test[category] = test[category].apply(lambda x: 1 if x in ['Yes','Female'] else 0)\n",
    "\n",
    "  return train, val, test\n",
    "\n",
    "def __preprocess_data(train, val, test):\n",
    "    print(f'before: {train.shape} / {test.shape}')\n",
    "    # 필요없는 컬럼 제거\n",
    "    __process_drop(train, val, test)\n",
    "    train, val, test = __fill_na(train,val,test)\n",
    "\n",
    "    # 범주형 처리\n",
    "    #train, val, test = __preprocess_label_encoding(train, val, test)\n",
    "    return __preprocess_dummy_encoding(train, val, test)\n",
    "\n",
    "\n",
    "@reset_seeds\n",
    "def preprocess_dataset():\n",
    "    # 데이터 로드\n",
    "    df_raw = __load_data()\n",
    "    # 데이터 분리\n",
    "    train_val, test = train_test_split(df_raw, test_size=0.1, stratify=df_raw['Churn'])\n",
    "    train, val = train_test_split(train_val, test_size=0.1, stratify=train_val['Churn'])\n",
    "    # 데이터 전처리\n",
    "    train, val, test = __preprocess_yn(train, val, test)\n",
    "    train, val, test = __preprocess_data(train, val, test)\n",
    "\n",
    "    # features, target 분리\n",
    "    return __preprocess_resample(train, val, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFnzkf5eMYDv"
   },
   "source": [
    "## model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1740367461321,
     "user": {
      "displayName": "박예슬 (Yeseul Park)",
      "userId": "06789135698482112547"
     },
     "user_tz": -540
    },
    "id": "L7u3EsKKt_ET",
    "outputId": "4c30e1cf-d646-4781-f1bf-2834485cb899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting service/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile service/model.py\n",
    "\n",
    "from lightgbm import LGBMClassifier, plot_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from utils import reset_seeds\n",
    "\n",
    "# 모델 생성 후 리턴\n",
    "@reset_seeds\n",
    "def get_model(hp:dict=None, model_nm:str=None):\n",
    "    if not hp:\n",
    "        hp = {\"verbose\":-1} # warning 로그 제거\n",
    "\n",
    "    if not model_nm:\n",
    "        return RandomForestClassifier(verbose = False, max_depth = 10, min_samples_split = 10)\n",
    "    elif model_nm == \"LGBMClassifier\":\n",
    "        return LGBMClassifier(**hp)\n",
    "    elif model_nm == \"RandomForestClassifier\":\n",
    "        return RandomForestClassifier(verbose = False)\n",
    "    elif model_nm == \"XGBoost\":\n",
    "        return XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bjRPI6lMdWO"
   },
   "source": [
    "## run_model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1740367143999,
     "user": {
      "displayName": "박예슬 (Yeseul Park)",
      "userId": "06789135698482112547"
     },
     "user_tz": -540
    },
    "id": "kCSZ_yByww-F",
    "outputId": "66983801-afd9-4473-b245-3aff285467ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting service/run_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile service/run_model.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from dataset import preprocess_dataset\n",
    "from model import get_model\n",
    "from utils import reset_seeds\n",
    "from config import MODEL_PATH, MODEL_DIR, N_FOLDS\n",
    "\n",
    "def get_cross_validation(shuffle:bool=True, is_kfold:bool=True, n_splits:int=N_FOLDS):\n",
    "    if is_kfold:\n",
    "      return KFold(n_splits=n_splits, shuffle=shuffle)\n",
    "    else:\n",
    "      return StratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "\n",
    "def run_cross_validation(my_model, x_train, y_train, cv, is_kfold:bool=True):\n",
    "    n_iter = 0\n",
    "    f1_lst = []\n",
    "    if is_kfold:\n",
    "        cross_validation = cv.split(x_train)\n",
    "    else:\n",
    "        cross_validation = cv.split(x_train, y_train)\n",
    "\n",
    "    for train_index, valid_index in cross_validation:\n",
    "      n_iter += 1\n",
    "      # 학습용, 검증용 데이터 구성\n",
    "      train_x, valid_x = x_train.iloc[train_index], x_train.iloc[valid_index]\n",
    "      train_y, valid_y = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "      # 학습\n",
    "      my_model.fit(train_x, train_y)\n",
    "      # 예측\n",
    "      y_pred = my_model.predict(valid_x)\n",
    "      # 평가\n",
    "      f1 = np.round(f1_score(valid_y, y_pred), 4)\n",
    "      f1_lst.append(f1)\n",
    "      print(f'{n_iter} 번째 K-fold F1: {f1}, 학습데이터 크기: {train_x.shape}, 검증데이터 크기: {valid_x.shape}')\n",
    "\n",
    "    return np.mean(f1_lst)\n",
    "\n",
    "def print_feature_importance(my_model, data):\n",
    "  feature_importance = my_model.feature_importances_\n",
    "  indices = np.argsort(feature_importance)[::-1]\n",
    "  print(\"Feature Ranking\")\n",
    "  for f in range(data.shape[1]):\n",
    "    print(f\"{data.columns[indices][f]} : {feature_importance[indices][f]}\")\n",
    "\n",
    "@reset_seeds\n",
    "def main():\n",
    "    # 데이터 로드 및 분류\n",
    "    X_train, X_test, y_train, y_test, _, _ = preprocess_dataset()\n",
    "    # 모델 생성\n",
    "    my_model = get_model()\n",
    "    # 교차 검증\n",
    "    is_Regression = False\n",
    "    my_cv = get_cross_validation(is_kfold=is_Regression)\n",
    "    # 모델 학습\n",
    "    f1 = run_cross_validation(my_model, X_train, y_train, my_cv, is_kfold=is_Regression)\n",
    "\n",
    "    # 피쳐 중요도\n",
    "    print_feature_importance(my_model, X_train)\n",
    "\n",
    "    # 테스트 데이터 예측\n",
    "    y_pred_main = my_model.predict(X_test)\n",
    "\n",
    "    # 모델 저장\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    with open(MODEL_PATH, 'wb') as f:\n",
    "        pickle.dump(my_model, f)\n",
    "\n",
    "    print(f\"모델이 {MODEL_PATH}에 저장되었습니다.\")\n",
    "\n",
    "    return f1_score(y_test, y_pred_main)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "  result = main()\n",
    "  print(f\"테스트 스코어는 {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6angwQi_8Aq"
   },
   "source": [
    "## inference 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1740367144053,
     "user": {
      "displayName": "박예슬 (Yeseul Park)",
      "userId": "06789135698482112547"
     },
     "user_tz": -540
    },
    "id": "3N_1eib544zS",
    "outputId": "178b4ca8-47a5-4aab-da22-34892be56cd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting service/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile service/inference.py\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from dataset import preprocess_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from config import MODEL_PATH\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"저장된 모델을 로드합니다.\"\"\"\n",
    "    try:\n",
    "        with open(MODEL_PATH, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"모델을 {MODEL_PATH}에서 로드했습니다.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"모델 로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_and_submit():\n",
    "    \"\"\"테스트 데이터에 대한 예측을 수행하고 제출 파일을 생성합니다.\"\"\"\n",
    "    # 데이터 전처리\n",
    "    _, _, _, _, X_test, y_test = preprocess_dataset()\n",
    "\n",
    "    # 모델 로드\n",
    "    model = load_model()\n",
    "    if model is None:\n",
    "        return False\n",
    "\n",
    "    # 예측 수행\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f'test f1 score : {f1}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_and_submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjeLjH2GqXuE"
   },
   "source": [
    "## 모델 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11258,
     "status": "ok",
     "timestamp": 1740367811823,
     "user": {
      "displayName": "박예슬 (Yeseul Park)",
      "userId": "06789135698482112547"
     },
     "user_tz": -540
    },
    "id": "Wk-eSYwW0Xj5",
    "outputId": "4bec9f50-da5a-476c-9a4c-a49f760b77a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python\n"
     ]
    }
   ],
   "source": [
    "!python3 service/run_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4720,
     "status": "ok",
     "timestamp": 1740367817873,
     "user": {
      "displayName": "박예슬 (Yeseul Park)",
      "userId": "06789135698482112547"
     },
     "user_tz": -540
    },
    "id": "bc8F1zKwAKRL",
    "outputId": "8c3b296c-ace1-4d24-efa8-120495f26f1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python\n"
     ]
    }
   ],
   "source": [
    "!python3 service/inference.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1KNZ59J0I4KLMFrwJIzVZGMqKCOXPd3dv",
     "timestamp": 1740319758562
    },
    {
     "file_id": "1fgsD3JwjIpdHSuGPKwuhqCp147zC6Kvx",
     "timestamp": 1739934323208
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
