import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss
from imblearn.over_sampling import SMOTE

# Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨
df = pd.read_csv("customer_churn_telecom_services.csv")
df.columns = [col.lower() for col in df.columns]

df["churn"] = df["churn"].map({"No": 0, "Yes": 1})
df["totalcharges"] = df["totalcharges"].replace(" ", np.nan).astype(float)
df.loc[:, "totalcharges"] = df["totalcharges"].fillna(df["totalcharges"].median())

label_cols = ["gender", "partner", "dependents", "phoneservice", "paperlessbilling"]
for col in label_cols:
    df[col] = LabelEncoder().fit_transform(df[col])

one_hot_cols = ["multiplelines", "internetservice", "onlinesecurity", "onlinebackup",
                "deviceprotection", "techsupport", "streamingtv", "streamingmovies",
                "contract", "paymentmethod"]
df = pd.get_dummies(df, columns=one_hot_cols, drop_first=True)

# 1ÎÖÑ ÎØ∏Îßå Í∞ÄÏûÖ Í≥†Í∞ù Ïó¨Î∂Ä Ï∂îÍ∞Ä
df["is_short_tenure"] = (df["tenure"] < 12).astype(int)

X = df.drop(columns=["churn"])
y = df["churn"]

smote = SMOTE(sampling_strategy="auto", random_state=42)

# Epoch ÌöüÏàò ÏÑ§Ï†ï
num_epochs = 5  # üî• Ïó¨Îü¨ Î≤à ÌïôÏäµ (Epoch Í∞úÎÖê Ï†ÅÏö©)

# Î™®Îç∏ Ï¥àÍ∏∞Ìôî (Ìïú Î≤àÎßå Ïã§Ìñâ)
models = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(eval_metric="logloss", random_state=42)
}

# Í≤∞Í≥º Ï†ÄÏû•ÏùÑ ÏúÑÌïú ÎîïÏÖîÎÑàÎ¶¨
results = {name: {"accuracy": [], "precision": [], "recall": [], "f1_score": [], "log_loss": []}
           for name in models.keys()}

# Î™®Îç∏ ÌïôÏäµ Î∞è ÌèâÍ∞Ä (Îç∞Ïù¥ÌÑ∞ÏÖãÎßå Î≥ÄÍ≤Ω, Î™®Îç∏ Ïú†ÏßÄ)
for epoch in range(num_epochs):
    print(f"\nüîÑ Epoch {epoch + 1} / {num_epochs}")

    # üî• Îß§ EpochÎßàÎã§ ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ ÏÉòÌîåÎßÅ (train_test_splitÏùÑ Îã§Ïãú Ïã§Ìñâ)
    X_resampled, y_resampled = smote.fit_resample(X, y)
    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2,
                                                        random_state=np.random.randint(0, 10000))

    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    for name, model in models.items():
        print(f"\nüîπ Training {name} (Epoch {epoch + 1})...")

        # üî• Í∏∞Ï°¥ Î™®Îç∏ Ïú†ÏßÄ & ÏÉà Îç∞Ïù¥ÌÑ∞Î°ú Ï∂îÍ∞Ä ÌïôÏäµ
        model.fit(X_train, y_train)  # Î™®Îç∏ÏùÑ Ïú†ÏßÄÌïú Ï±Ñ ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµ

        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        acc = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        loss = log_loss(y_test, y_pred_proba)

        results[name]["accuracy"].append(acc)
        results[name]["precision"].append(precision)
        results[name]["recall"].append(recall)
        results[name]["f1_score"].append(f1)
        results[name]["log_loss"].append(loss)

        # üî• ÌÑ∞ÎØ∏ÎÑêÏóê Í≤∞Í≥º Ï∂úÎ†• Ï∂îÍ∞Ä
        print(f"‚úÖ {name} Results (Epoch {epoch + 1}):")
        print(f"  Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, Log Loss: {loss:.4f}\n")

# üìå Í∞úÎ≥Ñ ÏßÄÌëúÎ≥Ñ ÏÑ±Îä• Î≥ÄÌôî ÏãúÍ∞ÅÌôî
metrics = ["accuracy", "precision", "recall", "f1_score", "log_loss"]
for metric in metrics:
    plt.figure(figsize=(10, 6))
    for name in models.keys():
        sns.lineplot(x=range(1, num_epochs + 1), y=results[name][metric], marker='o', linestyle='-', label=f"{name} {metric}")
    
    plt.xlabel("Epochs")
    plt.ylabel(f"{metric.capitalize()} Score")
    plt.title(f"Model {metric.capitalize()} Across Epochs")
    plt.legend()
    plt.grid(True)
    plt.show()

# üìå Feature Importance Î∂ÑÏÑù (RandomForest & XGBoost)
feature_importance = {}

for name, model in models.items():
    if hasattr(model, "feature_importances_"):  # RandomForest, XGBoost ÏßÄÏõê
        feature_importance[name] = model.feature_importances_

for name in feature_importance:
    sorted_idx = np.argsort(feature_importance[name])[::-1]
    plt.figure(figsize=(10, 6))
    sns.barplot(x=np.array(X.columns)[sorted_idx][:10], y=feature_importance[name][sorted_idx][:10])
    plt.xlabel("Features")
    plt.ylabel("Importance Score")
    plt.title(f"Top 10 Feature Importance ({name})")
    plt.xticks(rotation=45)
    plt.show()

# üìå SHAP Î∂ÑÏÑù (XGBoost Î™®Îç∏ ÏÇ¨Ïö©)
explainer = shap.Explainer(models["XGBoost"], X_train)
shap_values = explainer(X_test)

# üìå SHAP Summary Plot (Scatter)
shap.summary_plot(shap_values, X_test, feature_names=X.columns.tolist())  # üî• Feature Ïù¥Î¶Ñ Î∞òÏòÅ

# üìå SHAP Summary Plot (Bar Chart)
shap.summary_plot(shap_values, X_test, plot_type="bar", feature_names=X.columns.tolist())  # üî• Feature Ïù¥Î¶Ñ Î∞òÏòÅ

